hl|tc| Monte Carlo Methods
In Reinforcement Learning, Monte Carlo Methods only require experience - sample sequences of states, actions, & rewards from actual or simulated interactions with the environment. This is unlike Dynamic Programming, where complete probability distributions specifying environment's dynamics was required for the recurrence relations. The term "Monte Carlo" is used for any estimation method whose operation involves a significant random component. In this chapter it is used in a narrow sense - specifically for methods based on averaging complete returns, as opposed to methods that learn from partial returns. To have well defined returns, we consider only episodic tasks that eventually finish regardless of the actions taken.
When a complete environment model is not available, it is useful to estimate action values instead of state values for defining policies. First, we look at the following First-visit MC prediction algorithm for estimating \( q_\pi(s, a) \),
ms|i0| Input : a policy \( \pi \) to be evaluated
ms|i0| Initialize :
ms|i1| \( Q(s, a) \in \mathcal{R} \ \forall s \in \mathcal{S}, a \in \mathcal{A} \)
ms|i1| \( C(s, a) = 0 \ \forall s \in \mathcal{S}, a \in \mathcal{A} \)
ms|i0| For each Episode \([(S_0, A_0, R_1), (S_1, A_1, R_2),..., (S_{T-1}, A_{T-1}, R_T)] \) generated by following \( \pi \) :
ms|i1| initialize set of state-action pairs visited in the Episode : \( S_v = \phi \)
ms|i1| \( G \leftarrow 0 \)
ms|i1| For \( (s,a,r) \) in Episode [reversed] :
ms|i2| \( G \leftarrow r + \gamma G \)
ms|i2| If \( (s, a) \in S_v \) : continue
ms|i2| \( Q(s,a) = Q(s,a) + \frac{G - Q(s, a)}{C(s, a)} \)
ms|i2| \( C(s,a) = C(s, a) + 1 \)
ms|i2| insert \( (s,a) \in S_v \)
We can use this prediction approach as a part of Generalized Policy Iteration procedure. Similar to Value Iteration methods (in-place or otherwise), we can interleave Policy Improvement and Evaluation without blocking on exact Value function computation, which converges only asymptotically. For Monte Carlo methods, it is natural to alternate between Improvement and Evaluation on an episode-by-episode basis. The resulting algorithm is identical to policy evaluation described above with the following added steps,
ms|i0| Initialize :
ms|i1|..
ms|i1| \( \pi(s) \in \mathcal{A} \ \forall s \in \mathcal{S}\), arbitrarily
ms|i0| For each Episode .. :
ms|i1| ..
ms|i1| For \( (s, a, r) \) in Episode [reversed] :
ms|i2| ..
ms|i2| \( \pi(s) \leftarrow \arg \max_a Q(s, a) \)
The policy improvement theorem applies on the result of argmax operation as follows,
\( \begin{aligned} q_{\pi_k} (s, \pi_{k+1}(s)) &= q_{\pi_k} (s, \arg\max_a q_{\pi_k} (s, a)) \\ &= \max q_{\pi_k} (s, a) \\ &\ge q_{\pi_k} (s, \pi_k(s)) \\ &\ge v_{\pi_k} (s) \end{aligned} \)
The only gap in the iteration algorithm defined above is the assumption called Exploring Starts i.e. it does not explicitly ensure that all actions will be selected infinitely often.
hs| On-policy first-visit MC control for \(\epsilon\)-soft policies
One way we can ensure all action selection is by slightly changing the same policy used for both data generation and value optimization. \(\epsilon\)-greedy approaches are examples of such soft policies,
ms|i0| Algorithm parameter : small \( \epsilon >0 \)
ms|i0| Initialize :
ms|i1| ..
ms|i1| \(\pi \leftarrow\) an arbitrary \(\epsilon\)-soft policy
ms|i0| For each Episode .. :
ms|i1| ..
ms|i1| For \( (s,a,r) \) in Episode [reversed] :
ms|i2| ..
ms|i2| \( A^* \leftarrow \arg\max_a Q(s, a) \)
ms|i2| For \(a \in \mathcal{A}\) : \( \pi(a | s) \leftarrow \begin{cases} 1 - \epsilon + \frac{\epsilon}{|\mathcal{A}|}, & \text{if} \ a = A^* \\ \frac{\epsilon}{|\mathcal{A}|}, & \text{if} \ a \neq A^* \end{cases} \)
The following derivation proves base condition of policy improvement theorem for \( \epsilon \)-soft policies, where all actions are explored with probability \(\ge \epsilon \),
\( \begin{aligned} q_{\pi_k}(s, \pi_{k+1}(s)) &= \frac{\epsilon}{|\mathcal{A}|} \sum_{a \in \mathcal{A}} q_{\pi_k}(s, a) + (1 - \epsilon) \max_{a \in \mathcal{A}} q_{\pi_k}(s,a) \\ &\ge \frac{\epsilon}{|\mathcal{A}|} \sum_{a \in \mathcal{A}} q_{\pi_k}(s, a) + (1 - \epsilon) \sum_{a \in \mathcal{A}} \frac{\pi_k(s, a) - \frac{\epsilon}{|\mathcal{A}|}}{1 - \epsilon} q_{\pi_k}(s, a) \\ &= \sum_{a \in \mathcal{A}} \pi_k(s, a) \ q_{\pi_k}(s, a) \\ &= v_{\pi_k}(s) \end{aligned} \)
The inequality holds because on the LHS we have a max-value over a set, while on the RHS we have weighted average of value from the same set with non-zero coefficients summing up to 1.
Note that the proof is trivial if we restrict our scope only to \( \epsilon \)-greedy policies described in the algorithm above, the exploration coefficients \( \epsilon/|\mathcal{A}| \) are identical for \( (\pi_k, \pi_{k+1}) \) and the only difference of greedy choice is by definition higher in \( \pi_{k+1} \).
To prove that this process terminates at an optimal \( \epsilon \)-soft policy, we can consider an environment where the \( \epsilon \)-soft behavior is enforced by embedding this property as a second layer of dynamics i.e. for every action, the environment emulates the reaction to a random action with \( \epsilon \)-probability, and reacts normally to the action otherwise. For this environment, the unique optimal policy must satisfy the following property,
\( \begin{aligned} \tilde{v}_\pi(s) &= \frac{\epsilon}{|\mathcal{A}|} \sum_{a \in \mathcal{A}} \tilde{q}_\pi(s, a) + (1 - \epsilon) \max_{a \in |\mathcal{A}|} \tilde{q}_\pi(s, a) \\ &= \frac{\epsilon}{|\mathcal{A}|} \sum_{a \in \mathcal{A}} \sum_{ s' \in \mathcal{S}} \sum_{r \in {\mathcal{R}}} p(s', r \ | \ s, a) [r + \gamma \tilde{v}_\pi(s')] + (1 - \epsilon) \max_{a \in \mathcal{A}} \sum_{s' \in \mathcal{S}} \sum_{r \in \mathcal{R}} p(s', r \ | \ s, a) [r + \gamma\tilde{v}_\pi(s')] \end{aligned} \)
These equations are identical to the termination conditions for \( \epsilon \)-soft policy improvement steps described previously. Thus, proving that the algorithm reaches a unique optimal solution among all possible \( \epsilon \)-soft policies.
hs| Off-Policy Prediction via Importance Sampling
In contrast to On-Policy method to ensure exploring starts, Off-Policy methods use a behavior policy \( b \) to generate episodes for training the target policy \( \pi \). The common technique for these methods is the importance sampling ratio, which measures relative probability of a trajectory between target & behavior policies,
\( \rho_{_{t:T-1}} = \Pi_{k=t}^{T-1}\frac{\pi(A_k | S_k) p(S_{k+1} | S_k, A_k)}{b(A_k | S_k) p(S_{k+1} | S_k, A_k)} = \Pi_{k=t}^{T-1} \frac{\pi(A_k | S_k)}{b(A_k | S_k)} \)
This ratio only is independent of environment dynamics, and can be directly used to convert expected returns from behavior to target policy.
Given \( G_t  \) values generated from \( b \) such that \( E[G_t| S_t=s] = v_b(s) \), we get \( E[\rho_{_{t:T-1}} G_t | S_t = s] = v_\pi(s) \)
For convenience, We sequence all the episodes such that new ones start at the immediate next timestamp after the previous ones. Use \( \mathcal{T}(s) \) to denote set of all timestamps in which state \( s \) is visited, \( T(t) \) as the first termination time after \( t \), and \( G_t \) as the returns from \( t \) to \( T(t) \). There are two ways to implement this sampling,
Ordinary Importance Sampling : \( V(s) = \frac{\sum_{t \in \mathcal{T}(s)} \rho_{_{t:T(t) - 1}} G_t}{| \mathcal{T}(s) |} \)
Weighted Importance Sampling : \( V(s) = \frac{\sum_{t \in \mathcal{T}(s)} \rho_{_{t:T(t) - 1}} G_t}{ \sum_{t \in \mathcal{T}(s)} \rho_{_{t:T(t) - 1}} } \)
Ordinary Importance Sampling simply scales the returns before averaging. Weighted Importance Sampling can be thought of as artifically increasing/decreasing the counts of sample returns in proportion to the relative probability. For first-visit prediction, the former can have high variance, while the latter can be considered biased towards behavior policy in a statistical sense. Both of these approaches can be considered biased for every-visit prediction. Weighted Importance Sampling and every-visit prediction are generally recommended due to faster convergence and simpler return tracking respectively.
